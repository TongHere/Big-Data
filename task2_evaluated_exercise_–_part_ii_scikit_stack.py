# -*- coding: utf-8 -*-
"""Task2_Evaluated Exercise – Part II: SciKit-Stack.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_n3aqgkvMnB2qvGLpZpe_atvmA8mfyeE

#1.Preliminary Steps 


1. Set up a jupyter notebook 
2. Upload the dataset 
3. Select the variables shown in the table above
"""

# Commented out IPython magic to ensure Python compatibility.
# Load the libraries and dataset
import pandas as pd 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
from collections import Counter
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
df = pd.read_csv('LoanStats.csv')
df.head()

"""#2. Data Understanding 
1. Analyze the variables in dataset selected
"""

df.head()

df.tail()

df.info()

df.describe()

"""Put dataset into dateframe"""

df = pd.DataFrame(df)

df.shape

"""After checking missing value, there is no missing value in dataset"""

print("Missing values distribution: ",df.isnull().mean())
print("")

df.isna().sum()

"""Checking variable by grouping with different catagorical variable"""

df.groupby(by="home_ownership").mean()

df.groupby(by="purpose").mean()

df["purpose"].value_counts()

df["home_ownership"].value_counts()

"""1. Mortgage has the highest ratios 
2. Grade B has highest value counts but G is the lowest
"""

#Define plot size
plt.figure(figsize=[6,6])

#Define column to use
data = df["home_ownership"].value_counts()
#Define labels
labels = ["MORTGAGE","NONE","OTHER","OWN","RENT"]
#Define color palette
colors = sns.color_palette('pastel')
#Create pie chart
plt.pie(data,labels=labels,colors=colors, autopct='%.0f%%')
plt.title("Proportion of home_ownership")
plt.show()

df.grade.value_counts().plot.bar()
print(df.grade.value_counts(), '\n')

"""Frequency table
1. Term 36 is higher than 60 
2. purpose - debt consolidation is the highest
3. Housepwnership - Rent is slightly higher than mortgage
"""

categorical_features = ['term','purpose','home_ownership','grade']
for col in categorical_features:
    counts = df[col].value_counts().sort_index()
    fig = plt.figure(figsize=(9, 6))
    ax = fig.gca()
    counts.plot.bar(ax = ax,color= 'orange')
    ax.set_title(col + 'counts',color= 'blue')
    ax.set_xlabel(col) 
    ax.set_ylabel("Frequency")
plt.show()

"""Interest Rates 
   1. Range between 10-15 is the highest
   2. Range over 15 starts decreasing
"""

#Frequency Distribution of Interest Rates
plt.figure(figsize=(15,6))
plt.suptitle('Frequency Distribution of Interest Rates', fontsize=14, fontweight="bold")
plt.subplot(1,2,1)
sns.distplot(df['int_rate'], color="orange", label="No.Words", bins=35, hist_kws={"alpha": 0.5,"rwidth":0.8})
plt.xlabel("Int_rate", fontsize=12)
plt.ylabel("Frequency", fontsize=12)

"""Frequency Loan amount  
 1. Loan amount is the highest between range 5000-10000 
 2. starts decreasing after 10000
"""

#Frequency Distribution of loan_amnt
plt.figure(figsize=(15,6))
plt.suptitle('Frequency Distribution of loan_amnt', fontsize=14, fontweight="bold")
plt.subplot(1,2,1)
sns.distplot(df['loan_amnt'], color="Green", label="No.Words", bins=35, hist_kws={"alpha": 0.5,"rwidth":0.8})

plt.xlabel("loan_amnt", fontsize=12)
plt.ylabel("Frequency", fontsize=12)

df["term"].value_counts()

plt.figure(figsize=[8,5])
sns.histplot(data=df,x="term")
plt.title("term")
plt.show()

numeric_features=['loan_amnt','annual_inc']
for col in numeric_features:
    fig = plt.figure(figsize=(9, 6))
    ax = fig.gca()
    feature = df[col]
    feature.hist(bins=100, ax = ax)
    ax.axvline(feature.mean(), color='green', linewidth=2)
    ax.axvline(feature.median(), color='blue',linewidth=2)
    ax.set_title(col)
plt.show()

plt.figure(figsize = (12, 6))
ax = sns.boxplot(x='purpose', y='loan_amnt', data=df)
plt.setp(ax.artists, alpha=.5, linewidth=2, edgecolor="k")
plt.xticks(rotation=45)

plt.figure(figsize = (12, 6))
ax = sns.boxplot(x='home_ownership', y='loan_amnt', data=df)
plt.setp(ax.artists, alpha=.5, linewidth=2, edgecolor="k")
plt.xticks(rotation=45)

plt.figure(figsize = (12, 6))
ax = sns.boxplot(x='grade', y='loan_amnt', data=df)
plt.setp(ax.artists, alpha=.5, linewidth=2, edgecolor="k")
plt.xticks(rotation=45)

sns.heatmap(df.corr(),annot=True)

"""#3. Data Preparation
1. Missing Values
2. Transformation of all categorical variables
3. Split into Test and Training Dataset
"""

scaler = MinMaxScaler()
df[['loan_amnt', 'annual_inc']] = scaler.fit_transform(df[['loan_amnt', 'annual_inc']])

df['term'] = df.term.astype('category')
df['term']  =  df['term'].cat.codes
df['grade'] = df.grade.astype('category')
df['grade']  =  df['grade'].cat.codes
df['home_ownership'] = df.home_ownership.astype('category')
df['home_ownership']  =  df['home_ownership'].cat.codes
df['purpose'] = df.purpose.astype('category')
df['purpose']  =  df['purpose'].cat.codes

X, y = df[['loan_amnt','term','grade','home_ownership','annual_inc','purpose']].values, df['int_rate'].values
print('Features:',X[:6], '\nLabels:', y[:6], sep='\n')

"""# Modeling

### Split data 70%-30% into training set and test set
"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)
print ('Training dataset: %d \nTest dataset: %d ' % (X_train.shape[0], X_test.shape[0]))

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import metrics
from sklearn import preprocessing

x = df.drop(columns = ['int_rate','loan_amnt','annual_inc'])
y = df['int_rate'] #Target Variable

print(x.shape, y.shape)

x = np.array(x)
y = np.array(y)

y = y.reshape(-1,1)
y.shape

#Scaling the Data

from sklearn.preprocessing import StandardScaler, MinMaxScaler

scaler = StandardScaler()
minmax = MinMaxScaler()

x = scaler.fit_transform(x)
y = scaler.fit_transform(y)

from sklearn.preprocessing import StandardScaler, MinMaxScaler

scaler = StandardScaler()
minmax = MinMaxScaler()

x = scaler.fit_transform(x)
y = scaler.fit_transform(y)

model_lr = LinearRegression()
model_lr.fit(X_train, y_train)
y_pred = model_lr.predict(X_test)
print("Training set score: {:.7f}".format(model_lr.score(X_train, y_train)))
print("Test set score: {:.7f}".format(model_lr.score(X_test, y_test)))
print("RMSE: {:.7f}".format(np.sqrt(metrics.mean_squared_error(y_test, y_pred))))

"""####'loan_amnt','term','grade','home_ownership','annual_inc','purpose'"""

model_lr.coef_

def calculate_residuals(model, features, label):
    predictions = model_lr.predict(features)
    df_result = pd.DataFrame({'Actual':label, 'Predicted':predictions})
    df_result['Residuals'] = abs(df_result['Actual']) - abs(df_result['Predicted'])
    return df_result

def linear_assumption(model, features, label):
    df_result = calculate_residuals(model, features, label)
    fig1, ax1 = plt.subplots(figsize=(12,8))
    ax1 = sns.regplot(x='Actual', y='Predicted', data=df_result, color='steelblue')
    line_coords = np.arange(df_result.min().min(), df_result.max().max())
    ax1 = plt.plot(line_coords, line_coords,  # X and y points
              color='indianred')

linear_assumption(model_lr, X_test, y_test)

"""####Residual Plot"""

#Residual Plot
df_result = calculate_residuals(model_lr, X_test, y_test)
fig2, ax2 = plt.subplots(figsize=(12,8))
ax2.scatter(x=df_result['Predicted'], y=df_result['Residuals'], color='steelblue')
plt.axhline(y=0, color='indianred')
ax2.set_ylabel('Residuals', fontsize=12)
ax2.set_xlabel('Predicted', fontsize=12)
plt.show()

"""###Achieved an Accuracy of 90% on the testing data using a Multiple Linear Regression Model"""

accuracy_lr = model_lr.score(X_test, y_test)
print(accuracy_lr)

"""####Achieved an Accuracy of 80% on the testing data using a DecisionTreeRegressor"""

from sklearn.tree import DecisionTreeRegressor

dr_model = DecisionTreeRegressor() #Instantiate an object 
dr_model.fit(X_train, y_train)

accuracy_dr = dr_model.score(X_test, y_test)
accuracy_dr

"""chieved an Accuracy of 91% on the testing data using a RandomForestRegressor"""

from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor(n_estimators=75, max_depth=25) #Instantiate an object 

#Try Experimenting with this n_estimators and max_depth parameters

rf_model.fit(X_train, y_train)

accuracy_rf = rf_model.score(X_test, y_test)
accuracy_rf

"""Hyperparameter Tuning / Grid Search """

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, r2_score

from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import mean_squared_error


# Use a Gradient Boosting algorithm with different hyperparameters
alg = [
     [{
         #hyperpamaters choosen: learning_rate and n_estimators
 'learning_rate': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],
  'n_estimators' : [5,10,15,20,25,30,35,40,45,50]
 }, GradientBoostingRegressor()]


              ]

for params,alg in alg:
  # Find the best hyperparameter combination to optimize the R2 metric
  score = make_scorer(r2_score)
  gridsearch = GridSearchCV(alg, params, scoring=score, cv=3, return_train_score=True)
  gridsearch.fit(X_train, y_train)
  print("Best parameter combination:", gridsearch.best_params_, "\n")

  # Get the best model
  model=gridsearch.best_estimator_
  print(model, "\n")

  # Evaluate the model using the test data
  predictions = model.predict(X_test)
  mse = mean_squared_error(y_test, predictions)
  print("MSE:", mse)
  rmse = np.sqrt(mse)
  print("RMSE:", rmse)
  r2 = r2_score(y_test, predictions)
  print("R2:", r2)

  # Plot predicted vs actual
  plt.scatter(y_test, predictions)
  plt.xlabel('Actual Labels')
  plt.ylabel('Predicted Labels')
  plt.title('Actual vs Predicted')
  # overlay the regression line
  z = np.polyfit(y_test, predictions, 1)
  p = np.poly1d(z)
  plt.plot(y_test,p(y_test), color='magenta')
  plt.show()
  
#Control for overfitting: campare training and test datasets
  clf=model.fit(X_train,y_train)
  y_pred=model.predict(X_test)
    
  print(f'Training Score for   {clf.score(X_train,y_train) * 100:.2f}' )
  print(f'Testing Score for  {clf.score(X_test,y_test) * 100:.2f}' )

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# jupyter nbconvert --to html /content/Task2_Evaluated_Exercise_–_Part_II_SciKit_Stack.ipynb